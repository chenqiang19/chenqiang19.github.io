<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.0.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    hostname: new URL('http://yoursite.com').hostname,
    root: '/',
    scheme: 'Mist',
    version: '7.5.0',
    exturl: false,
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false},
    copycode: {"enable":false,"show_result":false,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: 'search.xml',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    sidebarPadding: 40
  };
</script>

  <meta name="description" content="开放神经网络交换(Open Neural Network Exchange)一、概述ONNX是微软和Facebook提出的用来表示深度学习模型的开放格式，其定义了一组和环境、平台均无关的标准格式，来增强各种AI模型的可交互性。 所以针对目前流行的一些训练框架训练的模型，如TensorFlow、Pytorch、MxNet或Paddle等。训练完毕后都可以将这些框架存储的模型统一转换为ONNX这种统一">
<meta name="keywords" content="Model Compression and Acceleration">
<meta property="og:type" content="article">
<meta property="og:title" content="Open-Neural-Network-Exchange">
<meta property="og:url" content="http:&#x2F;&#x2F;yoursite.com&#x2F;2021&#x2F;07&#x2F;25&#x2F;Open-Neural-Network-Exchange&#x2F;index.html">
<meta property="og:site_name" content="编辑尼撑">
<meta property="og:description" content="开放神经网络交换(Open Neural Network Exchange)一、概述ONNX是微软和Facebook提出的用来表示深度学习模型的开放格式，其定义了一组和环境、平台均无关的标准格式，来增强各种AI模型的可交互性。 所以针对目前流行的一些训练框架训练的模型，如TensorFlow、Pytorch、MxNet或Paddle等。训练完毕后都可以将这些框架存储的模型统一转换为ONNX这种统一">
<meta property="og:locale" content="zh-CN">
<meta property="og:updated_time" content="2021-07-25T13:52:23.387Z">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://yoursite.com/2021/07/25/Open-Neural-Network-Exchange/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true,
    isPage: false,
    isArchive: false
  };
</script>

  <title>Open-Neural-Network-Exchange | 编辑尼撑</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="编辑尼撑" type="application/atom+xml"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">编辑尼撑</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <p class="site-subtitle">学无止境</p>
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类</a>

  </li>
        <li class="menu-item menu-item-books">

    <a href="/books/" rel="section"><i class="fa fa-fw fa-fas fa-book"></i>书籍阅读</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档</a>

  </li>
        <li class="menu-item menu-item-schedule">

    <a href="/schedule/" rel="section"><i class="fa fa-fw fa-calendar"></i>日程表</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>

</nav>
  <div class="site-search">
    <div class="popup search-popup">
    <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocorrect="off" autocapitalize="none"
           placeholder="搜索..." spellcheck="false"
           type="text" id="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result"></div>

</div>
<div class="search-pop-overlay"></div>

  </div>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2021/07/25/Open-Neural-Network-Exchange/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Qiang Chen">
      <meta itemprop="description" content="记录是忘记的第一助手.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="编辑尼撑">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Open-Neural-Network-Exchange
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2021-07-25 21:49:13 / 修改时间：21:52:23" itemprop="dateCreated datePublished" datetime="2021-07-25T21:49:13+08:00">2021-07-25</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Model-Compression-and-Acceleration/" itemprop="url" rel="index">
                    <span itemprop="name">Model Compression and Acceleration</span>
                  </a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="开放神经网络交换-Open-Neural-Network-Exchange"><a href="#开放神经网络交换-Open-Neural-Network-Exchange" class="headerlink" title="开放神经网络交换(Open Neural Network Exchange)"></a>开放神经网络交换(Open Neural Network Exchange)</h2><h4 id="一、概述"><a href="#一、概述" class="headerlink" title="一、概述"></a>一、概述</h4><p>ONNX是微软和Facebook提出的用来表示深度学习模型的<strong>开放</strong>格式，其定义了一组和环境、平台均无关的标准格式，来增强各种AI模型的可交互性。</p>
<p>所以针对目前流行的一些训练框架训练的模型，如TensorFlow、Pytorch、MxNet或Paddle等。训练完毕后都可以将这些框架存储的模型统一转换为ONNX这种统一的格式进行存储，从而扩展了模型对于不同平台或环境的移植性。ONNX文件不仅仅存储了神经网络模型的权重，同时也存储了模型的结构信息以及网络中每一层的输入输出和一些其它的辅助信息。ONNX中包括如下三个关键部分：</p>
<ul>
<li>可扩展的计算图模型：定义了通用的计算图中间表示法（Intermediate Representation）。  </li>
<li>内置运算符集内置操作符集：<code>ai.onnx</code>和<code>ai.onnx.ml</code>，<code>ai.onnx</code>是默认的操作符集，主要针对神经网络模型，<code>ai.onnx.ml</code>主要适用于传统非神经网络机器学习模型。 </li>
<li>标准数据类型的定义：包括张量（tensors）、序列（sequences）和映射（maps）。</li>
</ul>
<p>ONNX规范有两个官方变体，主要区别在与支持的类型和默认的操作符集。ONNX神经网络变体只使用张量作为输入和输出；而作为支持传统机器学习模型的<code>ONNX-ML</code>，还可以识别序列和映射，<code>ONNX-ML</code>为支持非神经网络算法扩展了ONNX操作符集。</p>
<h4 id="二、ProtoBuf简述"><a href="#二、ProtoBuf简述" class="headerlink" title="二、ProtoBuf简述"></a>二、ProtoBuf简述</h4><p>ONNX作为一个文件格式，自然需要一定的规则去读取想要的信息或者是写入需要保存的信息。ONNX使用的是<strong>Protobuf</strong>这个序列化数据结构去存储神经网络的权重信息。</p>
<p>Protobuf是一种轻便高效的结构化数据存储格式，可以用于结构化数据串行化，或者说序列化。它很适合做数据存储或数据交换格式。可用于通讯协议、数据存储等领域的语言无关、平台无关、可扩展的序列化结构数据格式。目前提供了 C++、Java、Python 三种语言的 API（摘自官方介绍）。</p>
<p>Protobuf协议是一个以<code>*.proto</code>后缀文件为基础的，这个文件描述了用户自定义的数据结构。</p>
<h4 id="三、ONNX格式分析"><a href="#三、ONNX格式分析" class="headerlink" title="三、ONNX格式分析"></a>三、ONNX格式分析</h4><p>ONNX中最核心的部分就是<code>onnx.proto</code>这个文件了，它定义了ONNX这个数据协议的规则和一些其它信息。地址在<code>https://github.com/onnx/onnx/blob/master/onnx/onnx.proto</code>。其中的一些关键字段为：</p>
<ul>
<li><code>ModelProto</code></li>
<li><code>GraphProto</code></li>
<li><code>NodeProto</code></li>
<li><code>ValueInfoProto</code></li>
<li><code>TensorProto</code></li>
<li><code>AttributeProto</code></li>
</ul>
<p>当我们加载了一个ONNX之后，我们获得的就是一个<code>ModelProto</code>，它包含了一些版本信息，生产者信息和一个<code>GraphProto</code>。在<code>GraphProto</code>里面又包含了四个<code>repeated</code>数组，它们分别是<code>node</code>(<code>NodeProto</code>类型)，<code>input</code>(<code>ValueInfoProto</code>类型)，<code>output</code>(<code>ValueInfoProto</code>类型)和<code>initializer</code>(<code>TensorProto</code>类型)，其中<code>node</code>中存放了模型中所有的计算节点，<code>input</code>存放了模型的输入节点，<code>output</code>存放了模型中所有的输出节点，<code>initializer</code>存放了模型的所有权重参数。</p>
<p>我们知道要完整的表达一个神经网络，不仅仅要知道网络的各个节点信息，还要知道它们的拓扑关系。这个拓扑关系在ONNX中是如何表示的呢？ONNX的每个计算节点都会有<code>input</code>和<code>output</code>两个数组，这两个数组是string类型，通过<code>input</code>和<code>output</code>的指向关系，我们就可以利用上述信息快速构建出一个深度学习模型的拓扑图。这里要注意一下，<code>GraphProto</code>中的<code>input</code>数组不仅包含我们一般理解中的图片输入的那个节点，还包含了模型中所有的权重。例如，<code>Conv</code>层里面的<code>W</code>权重实体是保存在<code>initializer</code>中的，那么相应的会有一个同名的输入在<code>input</code>中，其背后的逻辑应该是把权重也看成模型的输入，并通过<code>initializer</code>中的权重实体来对这个输入做初始化，即一个赋值的过程。</p>
<p>最后，每个计算节点中还包含了一个<code>AttributeProto</code>数组，用来描述该节点的属性，比如<code>Conv</code>节点或者说卷积层的属性包含<code>group</code>，<code>pad</code>，<code>strides</code>等等，每一个计算节点的属性，输入输出信息都详细记录在<code>https://github.com/onnx/onnx/blob/master/docs/Operators.md</code>。</p>
<h4 id="四、PyTorch转换模型到ONNX"><a href="#四、PyTorch转换模型到ONNX" class="headerlink" title="四、PyTorch转换模型到ONNX"></a>四、PyTorch转换模型到ONNX</h4><p>1、到处预训练的AlexNet到一个ONNX文件，名为alexnet.onnx</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">dummy_input = torch.randn(<span class="number">10</span>,<span class="number">3</span>,<span class="number">224</span>,<span class="number">224</span>,device=<span class="string">"cuda"</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">model = torchvision.models.alexnet(pretrained=<span class="literal">True</span>).cuda()</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">input_names = [ <span class="string">"actual_input_1"</span> ] + [ <span class="string">"learned_%d"</span> % i <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">16</span>) ]</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">output_names = [ <span class="string">"output1"</span> ]</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">torch.onnx.export(model, dummy_input, <span class="string">"alexnet.onnx"</span>, verbose=<span class="literal">True</span>, input_names=input_names, output_names=output_names)</span></pre></td></tr></table></figure>
<p>在导出的alexnet.onnx文件中包含了一个二进制的protocol buffer，其中包含了网络结构和模型的参数。设置<code>verbose=True</code>是为了让模型的人眼可读性更好。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment">#验证ONNX库</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="comment">#安装onnx</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">conda install -c conda-forge onnx</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="comment">#运行</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> onnx</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="comment">#加载onnx模型</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">model = onnx.load(<span class="string">"alexnet.onnx"</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"><span class="comment">#检查模型</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">onnx.checker.check_model(model)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"><span class="comment">#打印人类可读的图的表示</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">print(onnx.helper.printable_graph(model.graph))</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line"><span class="comment">#使用ONNX runtimes运行onnx导出模型</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> onnxruntime <span class="keyword">as</span> ort</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">ort_session = ort.InferenceSession(<span class="string">"alexnet.onnx"</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">outputs = ort_session.run(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">	<span class="literal">None</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">    &#123;<span class="string">"actual_input_1"</span>: np.random.randn(<span class="number">10</span>,<span class="number">3</span>,<span class="number">224</span>,<span class="number">224</span>).astype(np.float32)&#125;,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line">)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line">print(outputs[<span class="number">0</span>])</span></pre></td></tr></table></figure>
<p>在内部，torch.onnx.export() 需要 torch.jit.ScriptModule 而不是 torch.nn.Module。 如果传入的模型还不是 ScriptModule，export() 将使用跟踪对其进行转换。 </p>
<h5 id="1、跟踪-Tracing"><a href="#1、跟踪-Tracing" class="headerlink" title="1、跟踪(Tracing)"></a>1、跟踪(Tracing)</h5><p>如果用一个还不是<code>ScriptModule</code> 的模块调用<code>torch.onnx.export()</code>，它首先执行与<code>torch.jit.trace()</code> 等效的操作。它使用给定的 args 执行一次模型并记录在执行期间发生的所有操作。 这意味着如果您的模型是动态的，例如根据输入数据改变行为，导出的模型将不会捕获这种动态行为(因为PyTorch是动态执行的，下一步的数据需要根据上一步的结果获得，与TensorFlow执行的静态图不同，所以转换PyTorch模型到ONNX的DAG(<strong>有向无环图</strong>)的时候存在较大的差异性)。 同样，跟踪可能仅对<strong>特定输入大小有效</strong>。 我们建议检查导出的模型并确保操作符看起来合理。 <strong>跟踪将展开循环和 if 语句</strong>，导出与跟踪运行完全相同的静态图。 <strong>如果要使用动态控制流导出模型，则需要使用脚本。</strong> 上面展开循环和if语句后，如果输入的数据不同而选择不同的分支，则需要单独处理这种情况。</p>
<h5 id="2、脚本-scripting"><a href="#2、脚本-scripting" class="headerlink" title="2、脚本(scripting)"></a>2、脚本(scripting)</h5><p>通过脚本编译模型可以保留动态控制流，并且对不同大小的输入有效。 要使用脚本： </p>
<ul>
<li>Use <a href="https://pytorch.org/docs/master/generated/torch.jit.script.html#torch.jit.script" target="_blank" rel="noopener"><code>torch.jit.script()</code></a> to produce a <code>ScriptModule</code>.</li>
<li>Call <code>torch.onnx.export()</code> with the <code>ScriptModule</code> as the model, and set the <code>example_outputs</code> arg. This is required so that the types and shapes of the outputs can be captured without executing the model.<a href="https://pytorch.org/docs/master/onnx.html#id5" target="_blank" rel="noopener">参考</a></li>
</ul>
<h5 id="3、一些需要避免的坑"><a href="#3、一些需要避免的坑" class="headerlink" title="3、一些需要避免的坑"></a>3、一些需要避免的坑</h5><ol>
<li><p>避免使用 NumPy 和内置 Python 类型 </p>
<p>PyTorch 模型可以使用 NumPy 或 Python 类型和函数编写，但在跟踪过程中，任何 NumPy 或 Python 类型（而不是 torch.Tensor）的变量都会转换为常量，如果这些值应根据输入而改变，那么该操作将产生错误的结果。 </p>
</li>
<li><p>避免 Tensor.data </p>
<p>使用 Tensor.data 字段会产生不正确的轨迹，因此会产生不正确的 ONNX 图。 改用 torch.Tensor.detach() 。</p>
</li>
</ol>
<h5 id="4、一些限制"><a href="#4、一些限制" class="headerlink" title="4、一些限制"></a>4、一些限制</h5><ul>
<li>仅 torch.Tensors和数字类型（例如 float、int）可以简单地转换为 torch.Tensors 。以及这些类型的元组和列表支持作为模型输入或输出。 在跟踪模式下接受 dict 和 str 输入和输出，但是： <ul>
<li>任何依赖于 dict 或 str 输入值的计算都将替换为在一次跟踪执行期间看到的常量值。 </li>
<li>任何作为 dict 的输出都将被其值的扁平序列无声地替换（键将被删除）。 例如。 {“foo”: 1, “bar”: 2} 变成 (1, 2)。 </li>
</ul>
</li>
<li>由于 ONNX 对嵌套序列的支持有限，脚本模式不支持某些涉及元组和列表的操作。 特别是不支持将元组附加到列表中。 在跟踪模式下，嵌套序列将在跟踪过程中自动展平。 </li>
</ul>
<h5 id="5、运算符实现的差异"><a href="#5、运算符实现的差异" class="headerlink" title="5、运算符实现的差异"></a>5、运算符实现的差异</h5><p>由于操作符实现的差异，在不同的运行时运行导出的模型可能会产生不同的结果，或者与 PyTorch 不同。 通常，这些差异在数值上很小，因此只有当您的应用程序对这些小差异敏感时，才应该考虑这一点。 </p>
<h5 id="6、不受支持的张量索引模式"><a href="#6、不受支持的张量索引模式" class="headerlink" title="6、不受支持的张量索引模式"></a>6、不受支持的张量索引模式</h5><h5 id="7、添加对运算符的支持"><a href="#7、添加对运算符的支持" class="headerlink" title="7、添加对运算符的支持"></a>7、添加对运算符的支持</h5><p>导出包含不受支持的运算符的模型时，您将看到如下错误消息： </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">RuntimeError: ONNX export failed: Couldn&#39;t export operator foo</span></pre></td></tr></table></figure>
<p>发生这种情况时，您需要更改模型以不使用该运算符，或者添加对运算符的支持。 具体实现根据需要查阅相关内容。</p>
<p>ATen 是 PyTorch 的内置张量库。</p>
<h4 id="五、ONNX-Runtime"><a href="#五、ONNX-Runtime" class="headerlink" title="五、ONNX Runtime"></a>五、ONNX Runtime</h4><p>ONNX Runtime 是一个以性能为中心的 ONNX 模型引擎，它可以跨多个平台和硬件（Windows、Linux 和 Mac 以及 CPU 和 GPU）高效地进行推理。 事实证明，ONNX 运行时可显着提高多个模型的性能。</p>
<p>1、安装 ONNX 和 ONNX Runtime。 </p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">pip install onnx</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">pip install onnxruntime</span></pre></td></tr></table></figure>
<p>获得 ONNX 和 ONNX Runtime 的二进制版本。 请注意，ONNX 运行时与 Python 版本 3.5<strong>到</strong>3.7 兼容。 </p>
<p>下面给出官网列出的一个超分辨率模型导出的事例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># Some standard imports</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> io</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.utils.model_zoo <span class="keyword">as</span> model_zoo</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.onnx</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># Super Resolution model definition in PyTorch</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn.init <span class="keyword">as</span> init</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SuperResolutionNet</span><span class="params">(nn.Module)</span>:</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, upscale_factor, inplace=False)</span>:</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">        super(SuperResolutionNet, self).__init__()</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">        self.relu = nn.ReLU(inplace=inplace)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">64</span>, (<span class="number">5</span>, <span class="number">5</span>), (<span class="number">1</span>, <span class="number">1</span>), (<span class="number">2</span>, <span class="number">2</span>))</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line">        self.conv2 = nn.Conv2d(<span class="number">64</span>, <span class="number">64</span>, (<span class="number">3</span>, <span class="number">3</span>), (<span class="number">1</span>, <span class="number">1</span>), (<span class="number">1</span>, <span class="number">1</span>))</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line">        self.conv3 = nn.Conv2d(<span class="number">64</span>, <span class="number">32</span>, (<span class="number">3</span>, <span class="number">3</span>), (<span class="number">1</span>, <span class="number">1</span>), (<span class="number">1</span>, <span class="number">1</span>))</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line">        self.conv4 = nn.Conv2d(<span class="number">32</span>, upscale_factor ** <span class="number">2</span>, (<span class="number">3</span>, <span class="number">3</span>), (<span class="number">1</span>, <span class="number">1</span>), (<span class="number">1</span>, <span class="number">1</span>))</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line">        self.pixel_shuffle = nn.PixelShuffle(upscale_factor)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">25</span></pre></td><td class="code"><pre><span class="line">        self._initialize_weights()</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">26</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">27</span></pre></td><td class="code"><pre><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">28</span></pre></td><td class="code"><pre><span class="line">        x = self.relu(self.conv1(x))</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">29</span></pre></td><td class="code"><pre><span class="line">        x = self.relu(self.conv2(x))</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">30</span></pre></td><td class="code"><pre><span class="line">        x = self.relu(self.conv3(x))</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">31</span></pre></td><td class="code"><pre><span class="line">        x = self.pixel_shuffle(self.conv4(x))</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">32</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">return</span> x</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">33</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">34</span></pre></td><td class="code"><pre><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_initialize_weights</span><span class="params">(self)</span>:</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">35</span></pre></td><td class="code"><pre><span class="line">        init.orthogonal_(self.conv1.weight, init.calculate_gain(<span class="string">'relu'</span>))</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">36</span></pre></td><td class="code"><pre><span class="line">        init.orthogonal_(self.conv2.weight, init.calculate_gain(<span class="string">'relu'</span>))</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">37</span></pre></td><td class="code"><pre><span class="line">        init.orthogonal_(self.conv3.weight, init.calculate_gain(<span class="string">'relu'</span>))</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">38</span></pre></td><td class="code"><pre><span class="line">        init.orthogonal_(self.conv4.weight)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">39</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">40</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create the super-resolution model by using the above model definition.</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">41</span></pre></td><td class="code"><pre><span class="line">torch_model = SuperResolutionNet(upscale_factor=<span class="number">3</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">42</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">43</span></pre></td><td class="code"><pre><span class="line"><span class="comment">#在导出模型之前调用 torch_model.eval() 或 torch_model.train(False) 很重要，以将模型转换为推理模式。 这是必需的，因为 dropout 或 batchnorm 等运算符在推理和训练模式下的行为不同。 </span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">44</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># Load pretrained model weights</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">45</span></pre></td><td class="code"><pre><span class="line">model_url = <span class="string">'https://s3.amazonaws.com/pytorch/test_data/export/superres_epoch100-44c6958e.pth'</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">46</span></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">1</span>    <span class="comment"># just a random number</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">47</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">48</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># Initialize model with the pretrained weights</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">49</span></pre></td><td class="code"><pre><span class="line">map_location = <span class="keyword">lambda</span> storage, loc: storage</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">50</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">51</span></pre></td><td class="code"><pre><span class="line">    map_location = <span class="literal">None</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">52</span></pre></td><td class="code"><pre><span class="line">torch_model.load_state_dict(model_zoo.load_url(model_url, map_location=map_location))</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">53</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">54</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># set the model to inference mode</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">55</span></pre></td><td class="code"><pre><span class="line">torch_model.eval()</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">56</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">57</span></pre></td><td class="code"><pre><span class="line"><span class="comment">#在 PyTorch 中导出模型是通过跟踪或脚本编写的。 上例将使用通过跟踪导出模型作为示例。 要导出模型，需要调用torch.onnx.export()函数。该操作将执行模型，记录用于计算输出的运算符的跟踪。 因为 export 运行模型，所以需要提供一个输入张量 x。 只要它是正确的类型和大小，其中的值可以是随机的。 请注意，除非指定为动态轴(dynamic_axes )，否则所有输入维度的导出 ONNX 图中的输入大小将是固定的。 在此示例中，我们使用 batch_size=1的输入导出模型，但随后在 torch.onnx.export() 的 dynamic_axes 参数中将第一个维度指定为动态。 因此，导出的模型将接受大小为 [batch_size, 1, 224, 224] 的输入，其中 batch_size 可以是可变的。 </span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">58</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">59</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># Input to the model</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">60</span></pre></td><td class="code"><pre><span class="line">x = torch.randn(batch_size, <span class="number">1</span>, <span class="number">224</span>, <span class="number">224</span>, requires_grad=<span class="literal">True</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">61</span></pre></td><td class="code"><pre><span class="line">torch_out = torch_model(x)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">62</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">63</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># Export the model</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">64</span></pre></td><td class="code"><pre><span class="line">torch.onnx.export(torch_model,               <span class="comment"># model being run</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">65</span></pre></td><td class="code"><pre><span class="line">                  x,                         <span class="comment"># model input (or a tuple for multiple inputs)</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">66</span></pre></td><td class="code"><pre><span class="line">                  <span class="string">"super_resolution.onnx"</span>,   <span class="comment"># where to save the model (can be a file or file-like object)</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">67</span></pre></td><td class="code"><pre><span class="line">                  export_params=<span class="literal">True</span>,        <span class="comment"># store the trained parameter weights inside the model file</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">68</span></pre></td><td class="code"><pre><span class="line">                  opset_version=<span class="number">10</span>,          <span class="comment"># the ONNX version to export the model to</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">69</span></pre></td><td class="code"><pre><span class="line">                  do_constant_folding=<span class="literal">True</span>,  <span class="comment"># whether to execute constant folding for optimization</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">70</span></pre></td><td class="code"><pre><span class="line">                  input_names = [<span class="string">'input'</span>],   <span class="comment"># the model's input names</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">71</span></pre></td><td class="code"><pre><span class="line">                  output_names = [<span class="string">'output'</span>], <span class="comment"># the model's output names</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">72</span></pre></td><td class="code"><pre><span class="line">                  dynamic_axes=&#123;<span class="string">'input'</span> : &#123;<span class="number">0</span> : <span class="string">'batch_size'</span>&#125;,    <span class="comment"># variable length axes</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">73</span></pre></td><td class="code"><pre><span class="line">                                <span class="string">'output'</span> : &#123;<span class="number">0</span> : <span class="string">'batch_size'</span>&#125;&#125;)</span></pre></td></tr></table></figure>
<p>但是在使用 ONNX Runtime 验证模型的输出之前，我们将使用 ONNX 的 API 检查 ONNX 模型。 首先，onnx.load(“super_resolution.onnx”) 将加载保存的模型并输出 onnx.ModelProto 结构。 然后，onnx.checker.check_model(onnx_model) 将验证模型的结构并确认模型具有有效的模式。 ONNX 图的有效性通过检查模型的版本、图的结构以及节点及其输入和输出来验证。 </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> onnx</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">onnx_model = onnx.load(<span class="string">"super_resolution.onnx"</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">onnx.checker.check_model(onnx_model)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="comment">#为了使用 ONNX Runtime运行模型，我们需要使用所选的配置参数（这里我们使用默认配置）为模型创建推理会话。 创建会话后，我们使用 run() api评估模型。 此调用的输出是一个列表，其中包含 ONNX Runtime 计算出的模型的输出。 </span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> onnxruntime</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">ort_session = onnxruntime.InferenceSession(<span class="string">"super_resolution.onnx"</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">to_numpy</span><span class="params">(tensor)</span>:</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">return</span> tensor.detach().cpu().numpy() <span class="keyword">if</span> tensor.requires_grad <span class="keyword">else</span> tensor.cpu().numpy()</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># compute ONNX Runtime output prediction</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">ort_inputs = &#123;ort_session.get_inputs()[<span class="number">0</span>].name: to_numpy(x)&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">ort_outs = ort_session.run(<span class="literal">None</span>, ort_inputs)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># compare ONNX Runtime and PyTorch results</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line">np.testing.assert_allclose(to_numpy(torch_out), ort_outs[<span class="number">0</span>], rtol=<span class="number">1e-03</span>, atol=<span class="number">1e-05</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line">print(<span class="string">"Exported model has been tested with ONNXRuntime, and the result looks good!"</span>)</span></pre></td></tr></table></figure>
<p><strong>使用 ONNX Runtime 在图像上运行模型</strong> </p>
<p>首先，让我们加载图像，使用标准 PIL python 库对其进行预处理。 请注意，此预处理是处理用于训练/测试神经网络的数据的标准做法。 首先，调整图像大小以适应模型输入的大小 (224x224)。 然后将图像分成 Y、Cb 和 Cr 分量。 这些分量代表灰度图像 (Y)，以及蓝色差 (Cb) 和红色差 (Cr) 色度分量。 Y 分量对人眼更敏感，对将要变换的这个分量感兴趣。 提取 Y 分量后，将其转换为张量，这将是模型的输入。 </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">img = Image.open(<span class="string">"./_static/img/cat.jpg"</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">resize = transforms.Resize([<span class="number">224</span>, <span class="number">224</span>])</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">img = resize(img)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">img_ycbcr = img.convert(<span class="string">'YCbCr'</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">img_y, img_cb, img_cr = img_ycbcr.split()</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">to_tensor = transforms.ToTensor()</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">img_y = to_tensor(img_y)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">img_y.unsqueeze_(<span class="number">0</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line"><span class="comment">#运行超分辨率模型在Onnx runtime</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">ort_inputs = &#123;ort_session.get_inputs()[<span class="number">0</span>].name: to_numpy(img_y)&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">ort_outs = ort_session.run(<span class="literal">None</span>, ort_inputs)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">img_out_y = ort_outs[<span class="number">0</span>]</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line"><span class="comment">#此时，上面是模型输出的一个张量。 现在，将处理模型的输出以从输出张量构造回最终输出图像，并保存图像。 此处的超分辨率模型的 PyTorch 实现采用了后处理步骤。</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line">img_out_y = Image.fromarray(np.uint8((img_out_y[<span class="number">0</span>] * <span class="number">255.0</span>).clip(<span class="number">0</span>, <span class="number">255</span>)[<span class="number">0</span>]), mode=<span class="string">'L'</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">25</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># get the output image follow post-processing step from PyTorch implementation</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">26</span></pre></td><td class="code"><pre><span class="line">final_img = Image.merge(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">27</span></pre></td><td class="code"><pre><span class="line">    <span class="string">"YCbCr"</span>, [</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">28</span></pre></td><td class="code"><pre><span class="line">        img_out_y,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">29</span></pre></td><td class="code"><pre><span class="line">        img_cb.resize(img_out_y.size, Image.BICUBIC),</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">30</span></pre></td><td class="code"><pre><span class="line">        img_cr.resize(img_out_y.size, Image.BICUBIC),</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">31</span></pre></td><td class="code"><pre><span class="line">    ]).convert(<span class="string">"RGB"</span>)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">32</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">33</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># Save the image, we will compare this with the output image from mobile device</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">34</span></pre></td><td class="code"><pre><span class="line">final_img.save(<span class="string">"./_static/img/cat_superres_with_ort.jpg"</span>)</span></pre></td></tr></table></figure>
<h4 id="六、ONNX结构分析"><a href="#六、ONNX结构分析" class="headerlink" title="六、ONNX结构分析"></a>六、ONNX结构分析</h4><p>ONNX将每一个网络的每一层或者说是每一个算子当作节点Node，再由这些Node去构建一个Graph，相当于是一个网络。最后将Graph和这个onnx模型的其他信息结合在一起，生成一个model，也就是最终的.onnx的模型。根据原始模型生成架构的不同，转换的过程可能也不相同，如：PyTorch和TensorFlow。</p>
<p>onnx网络查看工具<a href="https://netron.app/" target="_blank" rel="noopener">netron</a></p>
<h4 id="七、ONNX导出函数分析"><a href="#七、ONNX导出函数分析" class="headerlink" title="七、ONNX导出函数分析"></a>七、ONNX导出函数分析</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">torch.onnx.export(model, <span class="comment">#(torch.nn.Module, torch.jit.ScriptModule or torch.jit.ScriptFunction)将被导出的模型</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">				  args,  <span class="comment">#tuple or torch.Tensor 元组应包含模型输入，以便 model(*args) 是模型的有效调用。 任何非张量参数都将被硬编码到导出的模型中； 任何张量参数都将成为导出模型的输入，按照它们在元组中出现的顺序。 </span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">				  f,     <span class="comment">#类文件对象或或包含文件名的字符串。</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">			      export_params=<span class="literal">True</span>, <span class="comment">#如果为 True，将导出所有参数。 如果要导出未经训练的模型，请将其设置为 False。</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">				  verbose=<span class="literal">False</span>,    <span class="comment">#如果为 True，则打印正在导出到标准输出的模型的描述。 </span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">				  training=&lt;TrainingMode.EVAL: <span class="number">0</span>&gt;, <span class="comment">#TrainingMode.EVAL: 以推理模式导出模型；TrainingMode.PRESERVE: 如果 model.training 为 False，则在推理模式下导出模型，如果 model.training 为 True，则在训练模式下导出模型；TrainingMode.TRAINING:在训练模式下导出模型。 禁用可能干扰训练的优化。 </span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">				  input_names=<span class="literal">None</span>,    <span class="comment">#按顺序分配给图的输入节点的名称。</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">				  output_names=<span class="literal">None</span>,   <span class="comment">#按顺序分配给图形输出节点的名称。 </span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">				  aten=<span class="literal">False</span>,   <span class="comment">#operator_export_type=OperatorExportTypes.ONNX_ATEN，ATen部分有大量的代码是来声明和定义Tensor运算相关的逻辑的，实现Pytorch中tensor的加速</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">				  operator_export_type=<span class="literal">None</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">				  opset_version=<span class="literal">None</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">				  _retain_param_name=<span class="literal">True</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">				  do_constant_folding=<span class="literal">True</span>,<span class="comment">#应用常量折叠优化。 常量折叠将用预先计算的常量节点替换一些具有所有常量输入的操作。 </span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">				  example_outputs=<span class="literal">None</span>,    <span class="comment">#(T or a tuple of T, where T is Tensor or convertible to Tensor, default None)导出 ScriptModule 或 ScriptFunction 时必须提供，否则将被忽略。 用于在不跟踪模型执行的情况下确定输出的类型和形状。 单个对象被视为等同于一个元素的元组。 </span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">				  strip_doc_string=<span class="literal">True</span>,   <span class="comment">#不包括导出模型中的字段 doc_string`。 </span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">				  dynamic_axes=<span class="literal">None</span>,  <span class="comment">#默认情况下，导出的模型会将所有输入和输出张量的形状设置为与 args 中给出的形状（以及需要该 arg 时的 example_outputs）完全匹配。 要将张量轴指定为动态（即仅在运行时已知），请将 dynamic_axes 设置为具有模式的字典。KEY (str): 输入或输出名称。每个名称还必须在 input_names 或 output_names 中提供。VALUE (dict or list): 如果是字典，键是轴索引，值是轴名称。 如果是列表，则每个元素都是一个轴索引。 </span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">				  keep_initializers_as_inputs=<span class="literal">None</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">				  custom_opsets=<span class="literal">None</span>,</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">				  enable_onnx_checker=<span class="literal">True</span>, <span class="comment">#如果为 True，将运行 onnx 模型检查器以确保导出的模型是有效的 ONNX 模型。 </span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line">				  use_external_data_format=<span class="literal">False</span>)  <span class="comment">#如果为 True，则某些模型参数存储在外部数据文件中，而不是存储在 ONNX 模型文件本身中。 由于协议缓冲区的大小限制，无法将大于 2GB 的模型导出到一个文件中。 有关详细信息，请参阅 onnx.proto。 如果为 True，则参数 f 必须是指定模型位置的字符串。 外部数据文件将存储在与 f 相同的目录中。 除非 operator_export_type=OperatorExportTypes.ONNX，否则将忽略此参数。</span></span></pre></td></tr></table></figure>
<p><a href="https://pytorch.org/tutorials/advanced/super_resolution_with_onnxruntime.html" target="_blank" rel="noopener">参考-ONNX-PyTorch</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/86867138" target="_blank" rel="noopener">参考-知乎</a></p>
<p><a href="https://blog.csdn.net/xjm850552586/article/details/114898679" target="_blank" rel="noopener">参考-CSDN</a></p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Model-Compression-and-Acceleration/" rel="tag"># Model Compression and Acceleration</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2021/07/25/Gitlab-CI-Automation-Deployment/" rel="prev" title="Gitlab-CI-Automation-Deployment">
      <i class="fa fa-chevron-left"></i> Gitlab-CI-Automation-Deployment
    </a></div>
      <div class="post-nav-item">
    <a href="/2021/07/25/TensorRT/" rel="next" title="TensorRT">
      TensorRT <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          
    <div class="comments" id="gitalk-container"></div>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#开放神经网络交换-Open-Neural-Network-Exchange"><span class="nav-number">1.</span> <span class="nav-text">开放神经网络交换(Open Neural Network Exchange)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#一、概述"><span class="nav-number">1.0.1.</span> <span class="nav-text">一、概述</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#二、ProtoBuf简述"><span class="nav-number">1.0.2.</span> <span class="nav-text">二、ProtoBuf简述</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#三、ONNX格式分析"><span class="nav-number">1.0.3.</span> <span class="nav-text">三、ONNX格式分析</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#四、PyTorch转换模型到ONNX"><span class="nav-number">1.0.4.</span> <span class="nav-text">四、PyTorch转换模型到ONNX</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#1、跟踪-Tracing"><span class="nav-number">1.0.4.1.</span> <span class="nav-text">1、跟踪(Tracing)</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2、脚本-scripting"><span class="nav-number">1.0.4.2.</span> <span class="nav-text">2、脚本(scripting)</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3、一些需要避免的坑"><span class="nav-number">1.0.4.3.</span> <span class="nav-text">3、一些需要避免的坑</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#4、一些限制"><span class="nav-number">1.0.4.4.</span> <span class="nav-text">4、一些限制</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#5、运算符实现的差异"><span class="nav-number">1.0.4.5.</span> <span class="nav-text">5、运算符实现的差异</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#6、不受支持的张量索引模式"><span class="nav-number">1.0.4.6.</span> <span class="nav-text">6、不受支持的张量索引模式</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#7、添加对运算符的支持"><span class="nav-number">1.0.4.7.</span> <span class="nav-text">7、添加对运算符的支持</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#五、ONNX-Runtime"><span class="nav-number">1.0.5.</span> <span class="nav-text">五、ONNX Runtime</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#六、ONNX结构分析"><span class="nav-number">1.0.6.</span> <span class="nav-text">六、ONNX结构分析</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#七、ONNX导出函数分析"><span class="nav-number">1.0.7.</span> <span class="nav-text">七、ONNX导出函数分析</span></a></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Qiang Chen</p>
  <div class="site-description" itemprop="description">记录是忘记的第一助手.</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">87</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">14</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">23</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/chenaing19" title="GitHub → https://github.com/chenaing19" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="/765206494@qq.com" title="E-Mail → 765206494@qq.com"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Qiang Chen</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v4.0.0
  </div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">主题 �?<a href="https://mist.theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a> v7.5.0
  </div>
    <script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>



        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








        
      </div>
    </footer>
  </div>

  
  
  <script color='0,0,255' opacity='0.5' zIndex='-1' count='99' src="/lib/canvas-nest/canvas-nest.min.js"></script>
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script>
<script src="/js/schemes/muse.js"></script>
<script src="/js/next-boot.js"></script>



  




  <script src="/js/local-search.js"></script>








<script>
if (document.querySelectorAll('div.pdf').length) {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/pdfobject@2/pdfobject.min.js', () => {
    document.querySelectorAll('div.pdf').forEach(element => {
      PDFObject.embed(element.getAttribute('target'), element, {
        pdfOpenParams: {
          navpanes: 0,
          toolbar: 0,
          statusbar: 0,
          pagemode: 'thumbs',
          view: 'FitH'
        },
        PDFJS_URL: '/lib/pdf/web/viewer.html',
        height: element.getAttribute('height') || '500px'
      });
    });
  }, window.PDFObject);
}
</script>





  

  
      
<script type="text/x-mathjax-config">

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script>
<script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);
</script>

    

  

<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css">

<script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID: '9c00bb4a73071d490d0b',
      clientSecret: '0466125432b53cefbd6002b7ea866f7e15bbd9c8',
      repo: 'chenqiang19.github.io',
      owner: 'chenqiang19',
      admin: ['chenqiang19'],
      id: 'f8b11bfc9f11204060182d9d1bbbb6e4',
        language: '',
      distractionFreeMode: 'true'
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
</script>

  
  <!-- ҳ����С���� -->
  
  
    <script src="/js/cursor/love.min.js"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  



</body>
</html>
