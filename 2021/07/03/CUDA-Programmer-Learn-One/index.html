<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.0.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    hostname: new URL('http://yoursite.com').hostname,
    root: '/',
    scheme: 'Mist',
    version: '7.5.0',
    exturl: false,
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false},
    copycode: {"enable":false,"show_result":false,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: 'search.xml',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    sidebarPadding: 40
  };
</script>

  <meta name="description" content="CUDA编程学习笔记本笔记主要参考: 谭升大神的博客进行了部分关键知识的摘录，可能有些地方回去单独查找一些资料进行解释。 一、编程模型1、 简单介绍线程层级：CUDA编程是一个多线程编程，数个线程(Thread)组成一个线程块(Block)，所有线程块组成一个线程网格(Grid)。目前的GPU限制一个线程块中，最多可以安排1024个线程。由于32个相邻的线程会组成一个线程束(Thread Warp">
<meta name="keywords" content="CUDA Programmer">
<meta property="og:type" content="article">
<meta property="og:title" content="CUDA-Programmer-Learn-One">
<meta property="og:url" content="http:&#x2F;&#x2F;yoursite.com&#x2F;2021&#x2F;07&#x2F;03&#x2F;CUDA-Programmer-Learn-One&#x2F;index.html">
<meta property="og:site_name" content="编辑尼撑">
<meta property="og:description" content="CUDA编程学习笔记本笔记主要参考: 谭升大神的博客进行了部分关键知识的摘录，可能有些地方回去单独查找一些资料进行解释。 一、编程模型1、 简单介绍线程层级：CUDA编程是一个多线程编程，数个线程(Thread)组成一个线程块(Block)，所有线程块组成一个线程网格(Grid)。目前的GPU限制一个线程块中，最多可以安排1024个线程。由于32个相邻的线程会组成一个线程束(Thread Warp">
<meta property="og:locale" content="zh-CN">
<meta property="og:updated_time" content="2021-07-03T16:04:58.785Z">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://yoursite.com/2021/07/03/CUDA-Programmer-Learn-One/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true,
    isPage: false,
    isArchive: false
  };
</script>

  <title>CUDA-Programmer-Learn-One | 编辑尼撑</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="编辑尼撑" type="application/atom+xml"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">编辑尼撑</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <p class="site-subtitle">学无止境</p>
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档</a>

  </li>
        <li class="menu-item menu-item-schedule">

    <a href="/schedule/" rel="section"><i class="fa fa-fw fa-calendar"></i>日程表</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>

</nav>
  <div class="site-search">
    <div class="popup search-popup">
    <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocorrect="off" autocapitalize="none"
           placeholder="搜索..." spellcheck="false"
           type="text" id="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result"></div>

</div>
<div class="search-pop-overlay"></div>

  </div>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2021/07/03/CUDA-Programmer-Learn-One/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Qiang Chen">
      <meta itemprop="description" content="记录是忘记的第一助手.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="编辑尼撑">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          CUDA-Programmer-Learn-One
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-07-03 23:21:18" itemprop="dateCreated datePublished" datetime="2021-07-03T23:21:18+08:00">2021-07-03</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-07-04 00:04:58" itemprop="dateModified" datetime="2021-07-04T00:04:58+08:00">2021-07-04</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/CUDA-Programmer/" itemprop="url" rel="index">
                    <span itemprop="name">CUDA Programmer</span>
                  </a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="CUDA编程学习笔记"><a href="#CUDA编程学习笔记" class="headerlink" title="CUDA编程学习笔记"></a>CUDA编程学习笔记</h2><p>本笔记主要<a href="https://face2ai.com/program-blog/#GPU%E7%BC%96%E7%A8%8B%EF%BC%88CUDA%EF%BC%89" target="_blank" rel="noopener">参考: 谭升</a>大神的博客进行了部分关键知识的摘录，可能有些地方回去单独查找一些资料进行解释。</p>
<h4 id="一、编程模型"><a href="#一、编程模型" class="headerlink" title="一、编程模型"></a>一、编程模型</h4><p>1、 <strong>简单介绍线程层级：</strong>CUDA编程是一个多线程编程，数个线程(Thread)组成一个线程块(Block)，所有线程块组成一个线程网格(Grid)。目前的GPU限制一个<strong>线程块</strong>中，最多可以安排1024个线程。由于<strong>32个相邻的线程会组成一个线程束</strong>(Thread Warp)，而一个线程束中的线程会运行同样的指令。因此一般线程块中线程的数量被安排为32的倍数，选用256是比较合适的。</p>
<p>2、<strong>内核函数：</strong>CUDA每个线程执行的函数。关键字为global，返回值的关键字为void。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment">//kernal definition</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="function">__global__ <span class="keyword">void</span> <span class="title">vecAdd</span><span class="params">(<span class="keyword">float</span>* A, <span class="keyword">float</span>* B, <span class="keyword">float</span>* C)</span> </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">int</span> i=threadId.x;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">    C[i]=A[i]+B[i];</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">    vecAdd&lt;&lt;&lt;<span class="number">1</span>, N&gt;&gt;&gt;(A, B, C);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"><span class="function">__global__ <span class="keyword">void</span> <span class="title">matAdd</span><span class="params">(<span class="keyword">float</span> A[N][N], <span class="keyword">float</span> B[N][N], <span class="keyword">float</span> C[N][N])</span></span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">    <span class="comment">//blockDim代表当前线程块的尺寸</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">int</span> i = blockIdx.x * blockDim.x + threadIdx.x;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">int</span> j = blockIdx.y * blockDim.y + threadIdx.y;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">if</span>(i &lt; N &amp;&amp; j &lt; N)&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">        C[j][i] = A[j][i] + B[j][i]</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> main()&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line">    dim3 threadsPerBlock(<span class="number">16</span>,<span class="number">16</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line">    <span class="function">dim3 <span class="title">numBlocks</span><span class="params">(N / threadsPerBlock.x, N / threadsPerBlock.y)</span></span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line">    matAdd&lt;&lt;&lt;numBlocks, threadsPerBlock&gt;&gt;&gt;(A, B, C);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr></table></figure>
<p><strong>SIMT，相同指令，不同线程</strong></p>
<p>3、<strong>内存层级：</strong>一般主机端内存通过PCI-E总线与设备端内存交换数据。数据交换的速度等于PCI-E总线的速度。</p>
<ol>
<li>寄存器和本地内存绑定到了每个线程，其他线程无法访问。</li>
<li>同一个线程块内的线程，可以访问同一块共享内存。注意，即使两个线程块被调度到了同一个SM上，他们的共享内存也是隔离开的，不能互相访问。</li>
<li>网格中的所有线程都可以自由读写全局内存。</li>
<li>常量内存和纹理内存只能被CPU端修改，GPU内的线程只能读取数据。</li>
</ol>
<p>4、<strong>CPU/GPU混合编程：</strong></p>
<p>CPU和GPU的内存是独立的，如何在两者之间共享数据。</p>
<ul>
<li>主机端(Host，即CPU)执行串行代码，然后调用内核函数，让设备端(Device，即GPU)执行并行代码。如此交错执行。<ul>
<li>一、因此在运行内核函数前，主机端需要调用内存拷贝函数，将数据通过PCI-E总线拷贝到设备端。内核运行结束后，需要CPU再次调用内存拷贝函数，将数据拷回主机端内存。</li>
<li>二、使用统一编址，将设备端的内存和主机端内存编到一起。这样主机就不需要显式的调用函数将数据拷贝到设备端内存了。</li>
</ul>
</li>
<li>除了CPU/GPU交错执行代码的方式外，还可以通过使用事件(event)和流(stream)等方式，让CPU/GPU并行工作，提升整体的效率。</li>
</ul>
<p>5、<strong>计算能力：</strong>指不同的GPU版本，每个版本具有不同的特性，编程也会有所差异。CUDA的版本与计算能力没有关系，只是表示对不同架构的支持。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment">//CPU、OpenCV、CUDA分别执行图像灰度转换</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt;</span></span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;time.h&gt;</span></span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"opencv2/highgui.hpp"</span></span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"opencv2/opencv.hpp"</span></span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> cv;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"><span class="comment">//CPU转换</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"><span class="function">Mat <span class="title">cpuConvertGray</span><span class="params">(Mat src, <span class="keyword">int</span> <span class="built_in">height</span>, <span class="keyword">int</span> <span class="built_in">width</span>)</span></span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"><span class="function"></span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">    Mat outImg = zeros(imgHeight, imgWidth, CV_8UC1, Scalar(<span class="number">0</span>));</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> y=<span class="number">0</span>; y&lt;<span class="built_in">height</span>; ++y)&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> x=<span class="number">0</span>; x&lt;<span class="built_in">width</span>; ++x)&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">            outImg.at&lt;uchar&gt;(y,x) = <span class="number">0.2126</span> * (<span class="keyword">float</span>)src.at&lt;Vec3b&gt;(y,x)[<span class="number">2</span>] + \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">                <span class="number">0.7152</span> * (<span class="keyword">float</span>)src.at&lt;Vec3b&gt;(y,x)[<span class="number">1</span>] + \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">                <span class="number">0.0722</span> * (<span class="keyword">float</span>)src.at&lt;Vec3b&gt;(y,x)[<span class="number">0</span>];</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">        &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">return</span> outImg;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line"><span class="comment">//OpenCV转换</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line"><span class="function">Mat <span class="title">openCvtGray</span><span class="params">(Mat src)</span></span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line">    Mat out;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">25</span></pre></td><td class="code"><pre><span class="line">    cvtColor(src, out, COLOR_BGR2GRAY);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">26</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">return</span> out;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">27</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">28</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">29</span></pre></td><td class="code"><pre><span class="line"><span class="comment">//CUDA转换</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">30</span></pre></td><td class="code"><pre><span class="line"><span class="function">__global__ <span class="keyword">void</span> <span class="title">gpuConvertGray</span><span class="params">(uchar3* <span class="keyword">const</span> input, <span class="keyword">unsigned</span> <span class="keyword">char</span>* output, <span class="keyword">int</span> <span class="built_in">height</span>, <span class="keyword">int</span> <span class="built_in">width</span>)</span></span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">31</span></pre></td><td class="code"><pre><span class="line"><span class="function"></span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">32</span></pre></td><td class="code"><pre><span class="line">	<span class="keyword">const</span> <span class="keyword">unsigned</span> <span class="keyword">int</span> idx = blockIdx.x * blockDim.x + threadIdx.x;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">33</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">const</span> <span class="keyword">unsigned</span> <span class="keyword">int</span> idy = blockIdx.y * blockDim.y + threadIdy.y;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">34</span></pre></td><td class="code"><pre><span class="line">    </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">35</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">if</span>(idx &lt; <span class="built_in">width</span> &amp;&amp; idy &lt; <span class="built_in">height</span>)&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">36</span></pre></td><td class="code"><pre><span class="line">        uchar3 rgbImg = input[idy*<span class="built_in">width</span>+idx];</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">37</span></pre></td><td class="code"><pre><span class="line">        output[idy*<span class="built_in">width</span>+idx]=<span class="number">0.299f</span> * rgb.x + <span class="number">0.587f</span> * rgb.y + <span class="number">0.114f</span> * rgb.z;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">38</span></pre></td><td class="code"><pre><span class="line">    &#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">39</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">40</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">41</span></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">void</span>)</span> </span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">42</span></pre></td><td class="code"><pre><span class="line"><span class="function"></span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">43</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">clock_t</span> start, <span class="built_in">end</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">44</span></pre></td><td class="code"><pre><span class="line">    Mat srcImg = imread(imagePath);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">45</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">const</span> <span class="keyword">int</span> imgHeight = srcImg.rows;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">46</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">const</span> <span class="keyword">int</span> imgWidth = srcImg.cols;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">47</span></pre></td><td class="code"><pre><span class="line">    </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">48</span></pre></td><td class="code"><pre><span class="line">    Mat grayImg = zeros(imgHeight, imgWidth, CV_8UC1, Scalar(<span class="number">0</span>));</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">49</span></pre></td><td class="code"><pre><span class="line">    start = clock();</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">50</span></pre></td><td class="code"><pre><span class="line">    <span class="comment">//在CPU上进行图像灰度转化</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">51</span></pre></td><td class="code"><pre><span class="line">    grayImg = cpuConvertGray(srcImg, imgHeight, imgWidth);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">52</span></pre></td><td class="code"><pre><span class="line">    <span class="built_in">end</span> = clock();</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">53</span></pre></td><td class="code"><pre><span class="line">    <span class="built_in">printf</span>(<span class="string">"Cpu exec time is %.8f\n"</span>, (<span class="keyword">double</span>)(<span class="built_in">end</span>-start)/CLOCKS_PER_SEC);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">54</span></pre></td><td class="code"><pre><span class="line">   </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">55</span></pre></td><td class="code"><pre><span class="line">    Mat outImg = zeros(imgHeight, imgWidth, CV_8UC1, Scalar(<span class="number">0</span>));</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">56</span></pre></td><td class="code"><pre><span class="line">    uchar3* input;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">57</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">char</span>* output;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">58</span></pre></td><td class="code"><pre><span class="line">    </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">59</span></pre></td><td class="code"><pre><span class="line">    <span class="comment">//使用opencv转化</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">60</span></pre></td><td class="code"><pre><span class="line">    start = clock();</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">61</span></pre></td><td class="code"><pre><span class="line">    grayImg = openCvtGray(srcImg);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">62</span></pre></td><td class="code"><pre><span class="line">    <span class="built_in">end</span> = clock();</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">63</span></pre></td><td class="code"><pre><span class="line">    <span class="built_in">printf</span>(<span class="string">"OpenCV exec time is %.8f\n"</span>, (<span class="keyword">double</span>)(<span class="built_in">end</span>-start)/CLOCKS_PER_SEC);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">64</span></pre></td><td class="code"><pre><span class="line">    </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">65</span></pre></td><td class="code"><pre><span class="line">    <span class="comment">//在GPU上分配内存</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">66</span></pre></td><td class="code"><pre><span class="line">    cudaMalloc((<span class="keyword">void</span>**)&amp;input, <span class="built_in">height</span>*<span class="built_in">width</span>*<span class="keyword">sizeof</span>(uchar3));</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">67</span></pre></td><td class="code"><pre><span class="line">    cudaMalloc((<span class="keyword">void</span>**)&amp;output, <span class="built_in">height</span>*<span class="built_in">width</span>*<span class="keyword">sizeof</span>(<span class="keyword">unsigned</span> <span class="keyword">char</span>));</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">68</span></pre></td><td class="code"><pre><span class="line">    <span class="comment">//将图像数据从host拷贝到gpu上</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">69</span></pre></td><td class="code"><pre><span class="line">    cudaMemcpy(input, src.data, <span class="built_in">height</span>*<span class="built_in">width</span>*<span class="keyword">sizeof</span>(uchar3), cudaMemcpyHostToDevice);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">70</span></pre></td><td class="code"><pre><span class="line">    </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">71</span></pre></td><td class="code"><pre><span class="line">    <span class="function">dim3 <span class="title">threadsPerBlock</span><span class="params">(<span class="number">32</span>, <span class="number">32</span>)</span></span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">72</span></pre></td><td class="code"><pre><span class="line">    <span class="function">dim3 <span class="title">blocksPerGrid</span><span class="params">((imgWidth+threadsPerBlock.x<span class="number">-1</span>) / threadsPerBlock.x, \</span></span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">73</span></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="params">                       (imgHeight+threadsPerBlock.y<span class="number">-1</span>) / threadsPerBlock.y)</span></span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">74</span></pre></td><td class="code"><pre><span class="line">    </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">75</span></pre></td><td class="code"><pre><span class="line">    start = clock();</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">76</span></pre></td><td class="code"><pre><span class="line">    </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">77</span></pre></td><td class="code"><pre><span class="line">    <span class="comment">//启动内核</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">78</span></pre></td><td class="code"><pre><span class="line">    gpuConvertGray&lt;&lt;&lt;blocksPerGrid, threadsPerBlock&gt;&gt;&gt;(input, output, imgHeight, imgWidth);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">79</span></pre></td><td class="code"><pre><span class="line">    <span class="comment">//执行一个内核是一个异步操作，因此需要同步统计时间</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">80</span></pre></td><td class="code"><pre><span class="line">    cudaDeviceSynchronize();</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">81</span></pre></td><td class="code"><pre><span class="line">    <span class="built_in">end</span> = clock();</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">82</span></pre></td><td class="code"><pre><span class="line">    </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">83</span></pre></td><td class="code"><pre><span class="line">    <span class="built_in">printf</span>(<span class="string">"Cuda exec time is %.8f\n"</span>, (<span class="keyword">double</span>)(<span class="built_in">end</span>-start)/CLOCKS_PER_SEC);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">84</span></pre></td><td class="code"><pre><span class="line">    </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">85</span></pre></td><td class="code"><pre><span class="line">    cudaMemcpy(grayImg.data, output, imgHeight*imgWidth*<span class="keyword">sizeof</span>(<span class="keyword">unsigned</span> <span class="keyword">char</span>), cudaMemcpyDeviceToHost);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">86</span></pre></td><td class="code"><pre><span class="line">    cudaFree(input);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">87</span></pre></td><td class="code"><pre><span class="line">	cudaFree(output);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">88</span></pre></td><td class="code"><pre><span class="line">    </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">89</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">90</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr></table></figure>
<p>6、<strong>NVCC编译器编译CUDA程序</strong></p>
<p>NVCC提供了简单方便的接口，能够很好的同时处理主机端和设备端代码。</p>
<ul>
<li><p>离线编译</p>
<p><em>分离CUDA程序中的主机端代码(host code)和设备端代码(device code)</em>  将设备端代码编译成一种虚拟汇编文件(名为PTX)，再接着编译成二进制代码(名为cubin) ，将主机端代码中含有”&lt;&lt;&lt;&gt;&gt;&gt;”的代码(即内核调用)替换为CUDA运行库中的函数调用代码。之后NVCC会借助其他编译器(如gcc)将主机端代码编译出来，主机端代码和设备端代码被编译好后，nvcc会将两段代码链接起来。</p>
</li>
<li><p>在线编译</p>
<p>PTX是一个虚拟汇编文件。其形式虽然很像汇编，但里面的每一条指令实际上是一个虚拟的指令，与机器码无法对应。需要编译器或设备驱动程序将其翻译成对应平台的汇编/机器码才能运行。</p>
<p>如果在编译过程中，NVCC不将设备端代码编译为cubin文件，即二进制代码，而是停在PTX代码上。设备驱动(device  driver)会负责在运行时，使用PTX代码生成二进制代码。这个过程被称作在线编译(JIT Compilation, Just-In-Time  Compilation)。</p>
<p>在线编译必然会使得程序启动的时间延长，不过设备驱动程序会自动缓存编译出来的二进制代码(也被称作compute cache)。</p>
</li>
</ul>
<p>7、<strong>CUDA C运行库</strong></p>
<p>7.1 初始化：CUDA运行库没有显式的初始化函数，在调用第一个函数时会自动初始化(设备和版本管理函数不行)。初始化时，会产生一个全局可见的设备上下文(device context)。主机端代码调用了<code>cudaDeviceReset()</code>函数，则会销毁掉这个上下文。注意，销毁的上下文是主机端正在操纵的设备。如要更换，需要使用<code>cudaSetDevice()</code>来进行切换。</p>
<p>7.2 设备内存：</p>
<p>CUDA运行库提供了函数以分配/释放设备端的内存(全局内存+常量内存+纹理内存)，以及与主机端内存传输数据。</p>
<ul>
<li><p>线性存储(linear memory)：在GPU上用40位的地址线寻址</p>
<p>线性内存可以用<code>cudaMalloc()</code>分配，用<code>cudaFree()</code>释放，用<code>cudaMemcpy()</code>复制数据，用<code>cudaMemset()</code>赋值。</p>
<p>对于2D或3D数组，可以使用<code>cudaMallocPitch()</code>和<code>cudaMalloc3D()</code>来分配内存。这两个函数会自动padding，以满足内存对齐的要求，提高内存读写效率。</p>
</li>
<li><p>CUDA arrays—与纹理内存有关</p>
</li>
</ul>
<p>在设备内存中定义全局变量，则需要使用使用<code>__constant__</code>或<code>__device__</code>来修饰，并使用<code>cudaMemcpyToSymbol()</code>和<code>cudaMemcpyFromSymbol()</code>来读写。</p>
<p>实际上，当使用<code>__constant__</code>关键字时，是申请了一块常量内存；而使用<code>__device__</code>时，是普通的全局内存。因此<code>__device__</code>申请的内存需要申请，而<code>__constant__</code>不用。不管是全局内存，还是常量内存，需要用带有<code>Symbol</code>的函数拷贝。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">__constant__ <span class="keyword">float</span> constData[<span class="number">256</span>];</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">float</span> data[<span class="number">256</span>];</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">cudaMemcpyToSymbol(&amp;constData, data, <span class="keyword">sizeof</span>(data));</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">cudaMemcpyFromSymbol(&amp;data, constData, <span class="keyword">sizeof</span>(data));</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">__device__ <span class="keyword">float</span> devData;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">float</span> value = <span class="number">3.14f</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">cudaMemcpyToSymbol(devData, &amp;value, <span class="keyword">sizeof</span>(<span class="keyword">float</span>));</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">__device__ <span class="keyword">float</span>* devPtr;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">float</span>* ptr;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">cudaMalloc(&amp;ptr, <span class="number">256</span>*<span class="keyword">sizeof</span>(<span class="keyword">float</span>));</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">cudaMemcpyToSymbol(devPtr, &amp;ptr, <span class="keyword">sizeof</span>(ptr));</span></pre></td></tr></table></figure>
<p>8、<strong>共享内存</strong></p>
<p>不管是全局变量还是局部变量，都需要使用<code>__shared__</code>来修饰。不过需要注意的是，即使定义为全局变量，共享内存依旧只能被同一线程块内的线程可见。但是注意，并不是什么时候都可以使用共享内存来获取加速的。例如内核函数计算出来结果后，如果这个结果只需要传输回主机端，而不需要再次被用到时，直接写回全局内存会比较快。如果先写回共享内存，再写回全局内存，反而会比较缓慢。一般来讲，当需要频繁读写，或是有原子操作时，使用共享内存替代全局内存，会取得比较大的增益。</p>
<p><strong>共享内存只能为线程块内的线程共享。如果需要整个线程网格中线程都能访问，则需要全局内存或常量内存。</strong></p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment">//直方图统计</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">__shared__ <span class="keyword">unsigned</span> <span class="keyword">char</span> hist_shared[<span class="number">256</span>];</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="function">__global__ <span class="keyword">void</span> <span class="title">getGrayHistByCudaUsingSharedMem</span><span class="params">(<span class="keyword">unsigned</span> <span class="keyword">char</span> * <span class="keyword">const</span> grayData,</span></span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="params">                                               <span class="keyword">unsigned</span> <span class="keyword">int</span> * <span class="keyword">const</span> hist,</span></span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="params">                                               uint imgheight,</span></span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="params">                                               uint imgwidth)</span></span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="function"></span>&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">    </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr></table></figure>
<p>9、<strong>锁页内存</strong></p>
<p>锁页内存指的是主机端上不会被换出到虚拟内存(位于硬盘)上的内存。</p>
<p>锁页内存的分配与释放：在CUDA程序中，使用<code>cudaHostAlloc()</code>，可以分配锁页内存，使用<code>cudaFreeHost()</code>来释放锁页内存，或者使用<code>cudaHostRegister()</code>来将<code>malloc()</code>分配的内存指定为锁页内存。</p>
<p>10、<strong>合并写内存(Write-Combining Memory)</strong></p>
<p>11、<strong>异步并行执行</strong></p>
<ul>
<li><p>主机端/设备端并行：</p>
<ul>
<li>内核启动与执行</li>
<li>设备端内部传输数据</li>
<li>使用流或内存映射传输数据</li>
<li>设备端memset函数cudaMemset())</li>
</ul>
</li>
<li><p>内核并行执行</p>
<ul>
<li>计算能力2.x及以上的设备，支持多个内核函数同时执行。</li>
<li>执行多个内核函数，需要主机端不同的线程启动。如果一个线程依次启动多个内核，则这些内核会串行执行。同一线程的内核函数返回时会触发隐式的同步。</li>
<li>多个内核函数必须位于同一个CUDA上下文(CUDA context)上。不同CUDA上下文上的内核不能并行。</li>
</ul>
</li>
<li><p>数据传输和内核执行并行(需要使用锁页内存)</p>
<ul>
<li>一些设备支持数据传输(主机端/设备端、设备端/设备端)和内核执行并行，可通过检查<code>asyncEngineCount</code>来确认。</li>
</ul>
</li>
<li><p>数据传入核传出并行</p>
</li>
<li><p>流(stream)</p>
<p>可以通过<code>cudaStreamCreateWithPriority()</code>来在创建流时指定流的优先级。可以指定的优先级可由<code>cudaDeviceGetStreamPriorityRange()</code>来获得。</p>
<p>运行时，高优先级stream中的线程块不能打断正在执行的低优先级stream的线程块(即不是抢占式的)。但是当低优先级stream的线程块退出SM时，高优先级stream中的线程块会被优先调度进SM。</p>
<ul>
<li>在CUDA中，流(streams)指的是在GPU上一连串执行的命令。</li>
<li>不同的线程，可以向同一个流填入任务。</li>
<li>同一个流内的任务会按顺序执行。</li>
<li>同一设备上不同的流有可能并行，其执行顺序不会有保证。</li>
</ul>
</li>
</ul>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment">//流的创建和销毁</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">cudaStream_t stream[<span class="number">2</span>];</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;<span class="number">2</span>; i++)&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">    cudaStreamCreate(&amp;stream[i]);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;<span class="number">2</span>; i++)&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">    cudaStreamDestroy(stream[i]);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"><span class="comment">//当设备还在执行流中的任务，而用户调用cudaStreamDestroy()函数时，函数会立刻执行(不会阻塞)。之后，当流中的任务完成后，与流相关的资源会自动释放。</span></span></pre></td></tr></table></figure>
<p>12、<strong>显示同步(Explicit Synchronization)</strong></p>
<p><strong>cudaDeviceSynchronize():</strong>  直到<strong>所有线程</strong>向设备端的<strong>所有流</strong>的<strong>所有已送入指令</strong>完成，才会退出阻塞。</p>
<p><strong>cudaStreamSynchronize():</strong> 直到<strong>指定流</strong>的<strong>之前所有已送入指令</strong>完成，才会退出阻塞。</p>
<p><strong>cudaStreamWaitEvent():</strong> 需要stream和event作为输入参数。需要等待该函数等待的事件(Event)发生后，才能执行。</p>
<p>13、<strong>隐式同步(Implicit Synchronization)</strong></p>
<p>一般来讲，不同流内的命令可以并行。但是当任何一个流执行如下的命令时，情况例外，不能并行：</p>
<ul>
<li>锁页内存的分配</li>
<li>设备端内存分配  </li>
<li>设备端内存设置(memset) </li>
<li>设备内部拷贝 </li>
<li>NULL stream内的命令</li>
<li>L1 cache/共享内存空间的重新分配</li>
</ul>
<p>14、<strong>回调函数</strong></p>
<p>可以使用<code>cudaStreamAddCallback()</code>函数，向流中添加callback。该callback会在流中之前所有的任务完成后被调用。如果stream参数设为0，则代表之前的所有stream的任务执行完后就调用该callback。</p>
<p>回调函数和<code>cudaStreamWaitEvent()</code>一样，对于在加在callback之后的指令，必须等待callback<em>执行完成</em>后，才会继续执行。</p>
<p>回调函数中不能直接或间接的执行CUDA函数，否则会因为等待自己完成而造成死锁。</p>
<p>15、<strong>事件(Event)</strong></p>
<p>事件(Event)可以被压入流中以监视流的运行情况，或者用于精确计时。如果向stream 0压入事件，则当压入事件前向所有流压入的任务完成后，事件才被触发。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">cudaEvent_t start, <span class="built_in">stop</span>;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">cudaEventCreate(&amp;start);   <span class="comment">//创建</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">cudaEventCreate(&amp;<span class="built_in">stop</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">...</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">cudaEventDestroy(start);    <span class="comment">//销毁</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">cudaEventDestroy(<span class="built_in">stop</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"><span class="comment">//计算时间</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">cudaEventRecord(start, <span class="number">0</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;<span class="number">2</span>; i++)&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">    cudaMemcpyAsync(inputDev + i * <span class="built_in">size</span>, inputHost + i * <span class="built_in">size</span>, <span class="built_in">size</span>, cudaMemcpyHostToDevice, stream[i]);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">    MyKernel&lt;&lt;&lt;<span class="number">100</span>, <span class="number">512</span>, <span class="number">0</span>, stream[i]&gt;&gt;&gt;(outputDev + i * <span class="built_in">size</span>, inputDev + i * <span class="built_in">size</span>, <span class="built_in">size</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">    cudaMemcpyAsync(outputHost + i * <span class="built_in">size</span>, outputDev + i * <span class="built_in">size</span>, <span class="built_in">size</span>, cudaMemcpyDeviceToHost, stream[i]);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">cudaEventRecord(<span class="built_in">stop</span>, <span class="number">0</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">cudaEventSynchronize(<span class="built_in">stop</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">float</span> elapsedTime;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">cudaEventElapsedTime(&amp;elapsedTime, start, <span class="built_in">stop</span>);</span></pre></td></tr></table></figure>
<p>16、<strong>多设备系统</strong></p>
<ul>
<li>设备枚举</li>
<li>设备选择</li>
<li>流和事件的执行情况<ul>
<li><strong>内核启动</strong>：如果将内核压入不属于当前设备的流中，则内核会启动失败。也就是说，如果要向一个流中压入内核，必须先切换到流所在的设备。</li>
<li><strong>内存拷贝</strong>：如果对一个不属于当前设备的流进行内存拷贝工作，内存拷贝会成功。</li>
<li><strong>cudaEventRecord()</strong>：必须现将设备上下文切换过去，再向流压入事件。</li>
<li><strong>cudaEventElapsedTime()</strong>：计算时间差前，必须先切换设备。</li>
<li><strong>cudaEventSynchronize() and cudaEventQuery()</strong>：即使处于不同的设备，事件同步和事件查询依然有效。</li>
<li><strong>cudaStreamWaitEvent()</strong>：比较特殊，即使函数输入的流和事件不在同一个设备上，也能成功执行。也就是说，可以让流等待另一个设备上(当然当前设备也可以)的事件。这个函数可以用作多个设备间的同步。</li>
</ul>
</li>
<li>内存的访问</li>
<li>(设备间)对等内存访问：计算能力2.0及以上的设备支持设备间对等内存访问，这意味着两个GPU之间的传输和访问可以不经过主机端中转，速度会有提升。</li>
<li>(设备间)对等内存访问：对等设备的地址是统一编址的，可以使用<code>cudaMemcpyPeer()、cudaMemcpyPeerAsync()、cudaMemcpy3DPeer、cudaMemcpy3DPeerAsync()</code>来进行直接拷贝。无需先拷贝会主机端内存，再转到另一块卡上。</li>
</ul>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment">//设备枚举</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> deviceCount;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">cudaGetDeviceCount(&amp;deviceCount);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> device;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span>(device=<span class="number">0</span>; device&lt;deviceCount; device++)&#123;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">    cudaDeviceProp deviceProp;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">    cudaGetDeviceProperties(&amp;deviceProp, device);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">    <span class="built_in">printf</span>(<span class="string">"Device %d has compute capability %d.%d.\n"</span>, device, deviceProp.major, deviceProp.minor);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">&#125;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"><span class="comment">//设备选择</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line"><span class="comment">//使用cudaSetDevice()选择设备，当不选择时，默认使用设备0。</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line"><span class="comment">//注意，所有的内存分配、内核函数启动、流和事件的创建等，都是针对当前选择的设备的。</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">size_t</span> <span class="built_in">size</span> = <span class="number">1024</span> * <span class="keyword">sizeof</span>(<span class="keyword">float</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">cudaSetDevice(<span class="number">0</span>);   <span class="comment">// Set device 0 as current</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">float</span>* p0;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">cudaMalloc(&amp;p0, <span class="built_in">size</span>);  <span class="comment">// Allocate memory on device 0</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">MyKernel&lt;&lt;&lt;<span class="number">1000</span>, <span class="number">128</span>&gt;&gt;&gt;(p0);    <span class="comment">// Launch kernel on device 0</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">cudaSetDevice(<span class="number">1</span>);   <span class="comment">// Set device 1 as current</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">float</span>* p1;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line">cudaMalloc(&amp;p1, <span class="built_in">size</span>);  <span class="comment">// Allocate memory on device 1</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line">MyKernel&lt;&lt;&lt;<span class="number">1000</span>, <span class="number">128</span>&gt;&gt;&gt;(p1); <span class="comment">// Launch kernel on device 1</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line"><span class="comment">//设备间，对等内存拷贝</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">25</span></pre></td><td class="code"><pre><span class="line">cudaSetDevice(<span class="number">0</span>);   <span class="comment">// Set device 0 as current</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">26</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">float</span>* p0;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">27</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">size_t</span> <span class="built_in">size</span> = <span class="number">1024</span> * <span class="keyword">sizeof</span>(<span class="keyword">float</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">28</span></pre></td><td class="code"><pre><span class="line">cudaMalloc(&amp;p0, <span class="built_in">size</span>);  <span class="comment">// Allocate memory on device 0</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">29</span></pre></td><td class="code"><pre><span class="line">cudaSetDevice(<span class="number">1</span>);</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">30</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">float</span>* p1;</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">31</span></pre></td><td class="code"><pre><span class="line">cudaMalloc(&amp;p1, <span class="built_in">size</span>);  <span class="comment">// Allocate memory on device 1</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">32</span></pre></td><td class="code"><pre><span class="line">cudaSetDevice(<span class="number">0</span>);       <span class="comment">// Set Device 0 as Current</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">33</span></pre></td><td class="code"><pre><span class="line">MyKernel&lt;&lt;&lt;<span class="number">1000</span>, <span class="number">128</span>&gt;&gt;&gt;(p0);    <span class="comment">// Launch Kernel on Device 0</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">34</span></pre></td><td class="code"><pre><span class="line">cudaSetDevice(<span class="number">1</span>);               <span class="comment">// Set Device 1 as Current</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">35</span></pre></td><td class="code"><pre><span class="line">cudaMemcpyPeer(p1, <span class="number">1</span>, p0, <span class="number">0</span>, <span class="built_in">size</span>); <span class="comment">// Copy p0 to p1</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">36</span></pre></td><td class="code"><pre><span class="line">MyKernel&lt;&lt;&lt;<span class="number">1000</span>, <span class="number">128</span>&gt;&gt;&gt;(p1);        <span class="comment">// Launch Kernel on Device 1</span></span></pre></td></tr></table></figure>
<p>如果使用的是NULL stream，如果拷贝的双方中的任何一方，在设备拷贝前有任务未完成，则拷贝会被阻塞，直至任务完成。*  只有拷贝结束后，两者的后续任务才能继续执行。</p>
<h4 id="二、性能优化"><a href="#二、性能优化" class="headerlink" title="二、性能优化"></a>二、性能优化</h4><p>性能优化的原则：</p>
<ol>
<li>最大化并行，以提升资源利用率</li>
<li>优化内存排布，以最大化内存吞吐</li>
<li>最大化指令吞吐</li>
</ol>
<p>性能分析工具：CUDA profiler</p>
<p><strong>应用级别并行</strong>： 尽可能让主机端、设备端、PCI-E总线并行工作。对此可以使用异步CUDA函数，以及流(Stream)来实现。</p>
<p>同步操作，以及内存的共享会影响程序的并行性。因此需要仔细设计算法流程，尽量减少同步和内存共享。</p>
<p><strong>设备级别同步：</strong>可以通过流的方式，尽可能的让多个内核并行，提升利用率。</p>
<p><strong>处理器级别并行：</strong>延迟(latency)指的是线程束(从上一个动作开始)到它处于ready状态的时钟数。 例如线程束先提交了一个内存访问请求，然后等了400个时钟周期，内存管理系统才返回数据，线程束可以继续执行。这400个时钟周期称为延迟。</p>
<p>当一个线程束发生延迟时，线程束调度器(warp  scheduler)会将其他处于ready状态的线程束调度到SP上。等到延迟结束后，再将该线程调度回SP继续执行。这样一来，前一个线程束的延迟，就被另一个线程束的执行所隐藏了。 这一过程被称作延迟的隐藏(hidden latency)。  </p>
<p>隐藏延迟是GPU编程的核心概念。由于GPU具有巨大的寄存器空间，线程的切换不存在损耗。因此，通过向GPU上分配足够多的线程，可以让这些线程延迟互相交错，以起到隐藏延迟的作用，提高硬件利用率。</p>
<p><strong>最大化内存吞吐：</strong>主要手段就是少用低带宽的内存。首先要尽可能减少主机端和设备端间的设备传输(PCI-E，特别慢)，其次要尽可能减少全局内存的读写(快于PCI-E，但是相对于片内内存来说，还是挺慢的)；尽可能的使用片内的内存(寄存器、cache、共享内存)。</p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/CUDA-Programmer/" rel="tag"># CUDA Programmer</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2021/07/03/CUDA-Programmer-Learn-Zero/" rel="prev" title="CUDA-Programmer-Learn-Zero">
      <i class="fa fa-chevron-left"></i> CUDA-Programmer-Learn-Zero
    </a></div>
      <div class="post-nav-item">
    <a href="/2021/07/03/CUDA-Programmer-Learn-Two/" rel="next" title="CUDA-Programmer-Learn-Two">
      CUDA-Programmer-Learn-Two <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          
    <div class="comments" id="gitalk-container"></div>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#CUDA编程学习笔记"><span class="nav-number">1.</span> <span class="nav-text">CUDA编程学习笔记</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#一、编程模型"><span class="nav-number">1.0.1.</span> <span class="nav-text">一、编程模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#二、性能优化"><span class="nav-number">1.0.2.</span> <span class="nav-text">二、性能优化</span></a></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Qiang Chen</p>
  <div class="site-description" itemprop="description">记录是忘记的第一助手.</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">86</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">13</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">22</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/chenaing19" title="GitHub → https://github.com/chenaing19" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="/765206494@qq.com" title="E-Mail → 765206494@qq.com"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Qiang Chen</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v4.0.0
  </div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">主题 �?<a href="https://mist.theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a> v7.5.0
  </div>
    <script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>



        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








        
      </div>
    </footer>
  </div>

  
  
  <script color='0,0,255' opacity='0.5' zIndex='-1' count='99' src="/lib/canvas-nest/canvas-nest.min.js"></script>
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script>
<script src="/js/schemes/muse.js"></script>
<script src="/js/next-boot.js"></script>



  




  <script src="/js/local-search.js"></script>













  

  
      
<script type="text/x-mathjax-config">

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script>
<script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);
</script>

    

  

<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css">

<script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID: '9c00bb4a73071d490d0b',
      clientSecret: '0466125432b53cefbd6002b7ea866f7e15bbd9c8',
      repo: 'chenqiang19.github.io',
      owner: 'chenqiang19',
      admin: ['chenqiang19'],
      id: '60acc3ea46edb557b4aa32fbc3519a68',
        language: '',
      distractionFreeMode: 'true'
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
</script>

  
  <!-- ҳ����С���� -->
  
  
    <script src="/js/cursor/love.min.js"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  



</body>
</html>
